{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4762e9-af9f-4887-9c43-a888164ddd03",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db97cd-d88c-4fd6-b9ba-148779a7ad99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "# import stuff for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea80009-40ce-4623-a5ff-f49a9bc30696",
   "metadata": {},
   "source": [
    "### Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a90717f-7f67-456b-b416-10f9dca86461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of possible values to normalize to\n",
    "val_arr = [chr(32), chr(35), chr(37), chr(42), chr(43), chr(45), chr(46), chr(58), chr(61), chr(64)]\n",
    "lin = np.linspace(-1, 1, num=10)\n",
    "\n",
    "norm_dic = {}\n",
    "for k, lin in zip(val_arr, lin):\n",
    "    norm_dic[k] = lin\n",
    "norm_dic\n",
    "\n",
    "def to_number(x, normalize=True):\n",
    "    if normalize:\n",
    "        if (not type(x) is str and (x==None or np.isnan(x))):\n",
    "            return -1.;\n",
    "        return norm_dic[x]\n",
    "        \n",
    "    else:\n",
    "        if (not type(x) is str and (x==None or np.isnan(x))):\n",
    "            return 32;\n",
    "        return ord(x)\n",
    "\n",
    "class AsciiToDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, height=40, width=80):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx, normalize=True):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "\n",
    "        # create empty array\n",
    "        if normalize:\n",
    "            arr = np.zeros([40,80], dtype=\"float32\")\n",
    "        else:\n",
    "            arr = np.zeros([40,80], dtype=\"int\")\n",
    "\n",
    "        # indexes\n",
    "        i, j = 0, 0\n",
    "\n",
    "        # iterate through txt file and convert\n",
    "        with open(img_path) as f:\n",
    "            for line in f.readlines():\n",
    "                for ch in line: \n",
    "                    if not (ch == \"\\n\"):# leave out newline char\n",
    "                        arr[j][i] = to_number(ch, normalize)\n",
    "                    i += 1\n",
    "                j += 1\n",
    "                i = 0\n",
    "        \n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        return arr, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a909d8-0964-4b28-adf5-183307347a25",
   "metadata": {},
   "source": [
    "### Make datasets and -loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26d9b79c-4bf5-4389-a868-ca061f034041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotation_file = \"dataset/ascii.csv\"\n",
    "img_dir = \"dataset/w80_h40_ascii\"\n",
    "\n",
    "test_dataset = AsciiToDataset(annotation_file, img_dir)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True) #shuffle=false to debug\n",
    "\n",
    "train_features, train_labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b6496-c101-46f8-bfc7-3bbe236aa902",
   "metadata": {},
   "source": [
    "### Define Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fc28d8f-ac13-4b1c-b7ca-a73134329746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO conv1 is not correct: gets batch size first i think\n",
    "        self.conv1 = nn.Conv2d(64, 4, 5) # batch size =? input size, output size, kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # kernen size, stride\n",
    "        self.conv2 = nn.Conv2d(4, 64, 5)\n",
    "        self.fc1 = nn.Linear(119, 120)  # Wrong:\n",
    "                                            #12*7*12 is the dimension of input at this point:\n",
    "                                            # 7 = ((((40-4) /2) -4) /2) bc conv has kernel size 5\n",
    "                                            # and therefore shaves off 2 left and right and maxpool\n",
    "                                            # halves the size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"before: {x.shape}\")\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(f\"after conv1, relu and pool: {x.shape}\")\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(f\"after conv2, relu and pool: {x.shape}\")\n",
    "        x = torch.flatten(x, 1)  # flatten to 1D\n",
    "        #print(f\"after flattenin: {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(f\"after fc1 and relu: {x.shape}\")\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(f\"after fc2 and relu: {x.shape}\")\n",
    "        x = self.fc3(x)\n",
    "        #print(f\"after fc3: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61ecf0d1-b22b-429e-9314-f8558ba4cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ee085-79df-42c7-b9b0-147f1517bed6",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb74120c-2bdd-4c28-9ee4-08038a052107",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58c5eb-e68f-4c70-ad89-b2515f08883d",
   "metadata": {},
   "source": [
    "### Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d2a7e1c-58d0-4804-acc8-4293ec76b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard summary writer\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a502609-b1a8-4995-8fdd-a3dab77004e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 loss: 2.227235\n",
      "   40 loss: 2.231335\n",
      "   60 loss: 2.205677\n",
      "   80 loss: 2.235106\n",
      "  100 loss: 2.203714\n",
      "  120 loss: 2.231620\n",
      "  140 loss: 2.211777\n",
      "  160 loss: 2.224534\n",
      "  180 loss: 2.214506\n",
      "  200 loss: 2.221121\n",
      "  220 loss: 2.217701\n",
      "  240 loss: 2.212318\n",
      "  260 loss: 2.225044\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "    # print(f\"i: {i}, data: {data}\")\n",
    "    # break\n",
    "    inputs, labels = data\n",
    "\n",
    "    # zero parameter gradient #TODO: review optimizers\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    # write loss to runs for tensorboard\n",
    "    try:\n",
    "        writer.add_scalar(\"Loss\", loss, i) #see https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\n",
    "    except(NameError):\n",
    "        pass\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # statisticshttps://pytohttps://pytorch.org/tutorials//beginner/onnx/export_simple_model_to_onnx_tutorial.htmlrch.org/tutorials//beginner/onnx/export_simple_model_to_onnx_tutorial.html\n",
    "    # https://stackoverflow.com/questions/61092523/what-is-running-loss-in-pytorch-and-how-is-it-calculated\n",
    "    running_loss += loss.item()\n",
    "    if i%20 == 19:\n",
    "        print(f\"{i + 1:5d} loss: {running_loss / 20:.6f}\")\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e9f13-54fd-49f1-9c97-e182d9c2e54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab13e00-e779-4ea7-b982-6846ec81a926",
   "metadata": {},
   "source": [
    "# visualize Net architecture\n",
    "\n",
    "use [Netron](https://github.com/lutzroeder/netron) to visualize the net by exporting the net as an onnx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67665ac6-f952-42dc-90b9-2d975a75bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_net = Net()\n",
    "torch_input = torch.randn(64,80,40) #TODO: no idea of the dimensions are actually correct\n",
    "# export as onnx\n",
    "onnx_program = torch.onnx.dynamo_export(export_net, torch_input)\n",
    "onnx_program.save(\"classifier_net.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95c939-6bfa-4a8a-a1d0-0311aa4fdb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7a35b-d7df-4e7a-b6ed-c5e7ffb28573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
