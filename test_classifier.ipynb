{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4762e9-af9f-4887-9c43-a888164ddd03",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46db97cd-d88c-4fd6-b9ba-148779a7ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea80009-40ce-4623-a5ff-f49a9bc30696",
   "metadata": {},
   "source": [
    "### Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a90717f-7f67-456b-b416-10f9dca86461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(x):\n",
    "    if (not type(x) is str and (x==None or np.isnan(x))):\n",
    "        return 32;\n",
    "    return ord(x)\n",
    "\n",
    "class AsciiToDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, height=40, width=80):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "\n",
    "        # create empty array\n",
    "        arr = np.zeros([40,80], dtype=\"int\")\n",
    "\n",
    "        # indexes\n",
    "        i, j = 0, 0\n",
    "\n",
    "        # iterate through txt file and convert\n",
    "        with open(img_path) as f:\n",
    "            for line in f.readlines():\n",
    "                for ch in line: \n",
    "                    if not (ch == \"\\n\"):# leave out newline char\n",
    "                        arr[j][i] = to_int(ch)\n",
    "                    i += 1\n",
    "                j += 1\n",
    "                i = 0\n",
    "        \n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        return arr, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a909d8-0964-4b28-adf5-183307347a25",
   "metadata": {},
   "source": [
    "### Make datasets and -loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d9b79c-4bf5-4389-a868-ca061f034041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotation_file = \"dataset/ascii.csv\"\n",
    "img_dir = \"dataset/w80_h40_ascii\"\n",
    "\n",
    "test_dataset = AsciiToDataset(annotation_file, img_dir)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False) #shuffle=false to debug\n",
    "\n",
    "train_features, train_labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b6496-c101-46f8-bfc7-3bbe236aa902",
   "metadata": {},
   "source": [
    "### Define Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc28d8f-ac13-4b1c-b7ca-a73134329746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5) # input size, output size, kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # kernen size, stride\n",
    "        self.pool = nn.Conv2d(4, 12, 5)\n",
    "        self.fc1 = nn.Linear(12*7*17, 120)  #12*7*12 is the dimension of input at this point:\n",
    "                                            # 7 = ((((40-4) /2) -4) /2) bc conv has kernel size 5\n",
    "                                            # and therefore shaves off 2 left and right and maxpool\n",
    "                                            # halves the size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten to 1D\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ecf0d1-b22b-429e-9314-f8558ba4cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ee085-79df-42c7-b9b0-147f1517bed6",
   "metadata": {},
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74120c-2bdd-4c28-9ee4-08038a052107",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cd75b-e7ed-4f8b-984a-b3beb7baf593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889d42d-25da-4bde-93fd-cb774a1c0e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c016060-2db5-4741-b388-e37bf479fe15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
